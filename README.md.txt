numpy~=1.24.3
matplotlib~=3.7.2
h5py~=3.9.0
scikit-learn~=1.3.0
ujson~=5.4.0
scipy~=1.11.1

[![DOI](https://zenodo.org/badge/292225878.svg)](https://zenodo.org/doi/10.5281/zenodo.7780679)

***We expose this user-friendly platform for beginners who intend to start federated learning (FL) study.***

- *Now there are **31 traditional FL (tFL) or personalized FL (pFL) algorithms, 3 scenarios, and 14 datasets** in this platform.*

- *This platform can simulate scenarios using the 4-layer CNN on Cifar100 for **500 clients** on **one NVIDIA GeForce RTX 3090 GPU card** with only **5.08GB GPU memory** cost.*

- To simultaneously support statistical and model heterogeneity, please refer to our extended project **[Heterogeneous Federated Learning (HtFL)](https://github.com/TsingZ0/HtFL)**. 


Due to the frequent update, please download the **master branch** as the latest version.

The origin of the **statistical heterogeneity** phenomenon is the personalization of users, who generate the non-IID (not Independent and Identically Distributed) and unbalanced data. With statistical heterogeneity existing in the FL scenario, a myriad of approaches have been proposed to crack this hard nut. In contrast, the personalized FL (pFL) may take the advantage of the statistically heterogeneious data to learn the personalized model for each user. 

Thanks to [@Stonesjtu](https://github.com/Stonesjtu/pytorch_memlab/blob/d590c489236ee25d157ff60ecd18433e8f9acbe3/pytorch_memlab/mem_reporter.py#L185), this platform can also record the **GPU memory usage** for the model. By using the package [opacus](https://opacus.ai/), we introduce **DP (differential privacy)** into this platform (please refer to `./system/flcore/clients/clientavg.py` for example). Following [FedCG](https://www.ijcai.org/proceedings/2022/0324), we also introduce the **[DLG (Deep Leakage from Gradients)](https://papers.nips.cc/paper_files/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html) attack** and **PSNR (Peak Signal-to-Noise Ratio) metric** to evaluate the privacy-preserving ability of tFL/pFL algorithms (please refer to `./system/flcore/servers/serveravg.py` for example). *Now we can train on some clients and evaluate on other new clients by setting `args.num_new_clients` in `./system/main.py`. Note that not all the tFL/pFL algorithms support this feature.*


## Algorithms with code (updating)

> ### Traditional FL (tFL)

- **FedAvg** — [Communication-Efficient Learning of Deep Networks from Decentralized Data](http://proceedings.mlr.press/v54/mcmahan17a.html) *AISTATS 2017*

  ***Update-correction-based tFL***

- **SCAFFOLD** - [SCAFFOLD: Stochastic Controlled Averaging for Federated Learning](http://proceedings.mlr.press/v119/karimireddy20a.html) *ICML 2020*

  ***Regularization-based tFL***

- **FedProx** — [Federated Optimization in Heterogeneous Networks](https://proceedings.mlsys.org/paper/2020/hash/38af86134b65d0f10fe33d30dd76442e-Abstract.html) *MLsys 2020*
- **FedDyn** — [Federated Learning Based on Dynamic Regularization](https://openreview.net/forum?id=B7v4QMR6Z9w) *ICLR 2021*

  ***Model-splitting-based tFL***

- **MOON** — [Model-Contrastive Federated Learning](https://openaccess.thecvf.com/content/CVPR2021/html/Li_Model-Contrastive_Federated_Learning_CVPR_2021_paper.html) *CVPR 2021*

  ***Knowledge-distillation-based tFL***

- **FedGen** — [Data-Free Knowledge Distillation for Heterogeneous Federated Learning](http://proceedings.mlr.press/v139/zhu21b.html) *ICML 2021*

> ### Personalized FL (pFL)

- **FedMTL (not MOCHA)** — [Federated multi-task learning](https://papers.nips.cc/paper/2017/hash/6211080fa89981f66b1a0c9d55c61d0f-Abstract.html) *NeurIPS 2017*
- **FedBN** — [FedBN: Federated Learning on non-IID Features via Local Batch Normalization](https://openreview.net/forum?id=6YEQUn0QICG) *ICLR 2021*

  ***Meta-learning-based pFL***

- **Per-FedAvg** — [Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach](https://proceedings.neurips.cc/paper/2020/hash/24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html) *NeurIPS 2020*

  ***Regularization-based pFL***
  
- **pFedMe** — [Personalized Federated Learning with Moreau Envelopes](https://papers.nips.cc/paper/2020/hash/f4f1f13c8289ac1b1ee0ff176b56fc60-Abstract.html) *NeurIPS 2020*
- **Ditto** — [Ditto: Fair and robust federated learning through personalization](https://proceedings.mlr.press/v139/li21h.html) *ICML 2021*

  ***Personalized-aggregation-based pFL***

- **APFL** — [Adaptive Personalized Federated Learning](https://arxiv.org/abs/2003.13461) *2020* 
- **FedFomo** — [Personalized Federated Learning with First Order Model Optimization](https://openreview.net/forum?id=ehJqJQk9cw) *ICLR 2021*
- **FedAMP** — [Personalized Cross-Silo Federated Learning on non-IID Data](https://ojs.aaai.org/index.php/AAAI/article/view/16960) *AAAI 2021*
- **FedPHP** — [FedPHP: Federated Personalization with Inherited Private Models](https://link.springer.com/chapter/10.1007/978-3-030-86486-6_36) *ECML PKDD 2021*
- **APPLE** — [Adapt to Adaptation: Learning Personalization for Cross-Silo Federated Learning](https://www.ijcai.org/proceedings/2022/301) *IJCAI 2022*
- **FedALA** — [FedALA: Adaptive Local Aggregation for Personalized Federated Learning](https://ojs.aaai.org/index.php/AAAI/article/view/26330) *AAAI 2023* 

  ***Model-splitting-based pFL***

- **FedPer** — [Federated Learning with Personalization Layers](https://arxiv.org/abs/1912.00818) *2019*
- **LG-FedAvg** — [Think Locally, Act Globally: Federated Learning with Local and Global Representations](https://arxiv.org/abs/2001.01523) *2020*
- **FedRep** — [Exploiting Shared Representations for Personalized Federated Learning](http://proceedings.mlr.press/v139/collins21a.html) *ICML 2021*
- **FedRoD** — [On Bridging Generic and Personalized Federated Learning for Image Classification](https://openreview.net/forum?id=I1hQbx10Kxn) *ICLR 2022*
- **FedBABU** — [Fedbabu: Towards enhanced representation for federated image classification](https://openreview.net/forum?id=HuaYQfggn5u) *ICLR 2022*
- **FedGC** — [Federated Learning for Face Recognition with Gradient Correction](https://ojs.aaai.org/index.php/AAAI/article/view/20095/19854) *AAAI 2022*
- **FedCP** — [FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy](https://arxiv.org/pdf/2307.01217v2.pdf) *KDD 2023*
- **GPFL** — [GPFL: Simultaneously Learning Generic and Personalized Feature Information for Personalized Federated Learning](https://arxiv.org/pdf/2308.10279v3.pdf) *ICCV 2023*

  ***Knowledge-distillation-based pFL***

- **FedDistill** — [Federated Knowledge Distillation](https://www.cambridge.org/core/books/abs/machine-learning-and-wireless-communications/federated-knowledge-distillation/F679266F85493319EB83635D2B17C2BD#access-block) *2020*

- **FML** — [Federated Mutual Learning](https://arxiv.org/abs/2006.16765) *2020*

- **FedKD** — [Communication-efficient federated learning via knowledge distillation](https://www.nature.com/articles/s41467-022-29763-x) *Nature Communications 2022*

- **FedProto** — [FedProto: Federated Prototype Learning across Heterogeneous Clients](https://ojs.aaai.org/index.php/AAAI/article/view/20819) *AAAI 2022*

- **FedPCL (w/o pre-trained models)** — [Federated learning from pre-trained models: A contrastive learning approach](https://proceedings.neurips.cc/paper_files/paper/2022/file/7aa320d2b4b8f6400b18f6f77b6c1535-Paper-Conference.pdf) *NeurIPS 2022* 

- **FedPAC** — [Personalized Federated Learning with Feature Alignment and Classifier Collaboration](https://openreview.net/pdf?id=SXZr8aDKia) *ICLR 2023*

## Datasets and separation (updating)
For the ***label skew*** scenario, we introduce **8** famous datasets: **MNIST**, **Fashion-MNIST**, **Cifar10**, **Cifar100**, **AG_News**, **Sogou_News**, and **Tiny-ImageNet** (fetch raw data from [this site](http://cs231n.stanford.edu/tiny-imagenet-200.zip)), they can be easy split into **IID** and **non-IID** version. Since some codes for generating datasets such as splitting are the same for all datasets, we move these codes into `./dataset/utils/dataset_utils.py`. In **non-IID** scenario, 2 situations exist. The first one is the **pathological non-IID** scenario, the second one is **practical non-IID** scenario. In the **pathological non-IID** scenario, for example, the data on each client only contains the specific number of labels (maybe only 2 labels), though the data on all clients contains 10 labels such as MNIST dataset. In the **practical non-IID** scenario, Dirichlet distribution is utilized (please refer to this [paper](https://proceedings.neurips.cc/paper/2020/hash/18df51b97ccd68128e994804f3eccc87-Abstract.html) for details). We can input `balance` for the iid scenario, where the data are uniformly distributed. 

For the ***feature shift*** scenario, we use **3** datasets that are widely used in Domain Adaptation: **AmazonReview** (fetch raw data from [this site](https://drive.google.com/file/d/1QbXFENNyqor1IlCpRRFtOluI2_hMEd1W/view?usp=sharing)), **Digit5** (fetch raw data from [this site](https://drive.google.com/file/d/1PT6K-_wmsUEUCxoYzDy0mxF-15tvb2Eu/view?usp=share_link)), and **DomainNet**.

For the ***real-world (or IoT)*** scenario, we also introduce **3** naturally separated datasets: **Omniglot** (20 clients, 50 labels), **HAR (Human Activity Recognition)** (30 clients, 6 labels), **PAMAP2** (9 clients, 12 labels). For the details of datasets and FL algorithms in **IoT**, please refer to [my FL-IoT repo](https://github.com/TsingZ0/FL-IoT).

*If you need another data set, just write another code to download it and then using the utils.*

### Examples for **MNIST**
- MNIST
    ```
    cd ./dataset
    python generate_mnist.py iid - - # for iid and unbalanced scenario
    # python generate_mnist.py iid balance - # for iid and balanced scenario
    # python generate_mnist.py noniid - pat # for pathological noniid and unbalanced scenario
    # python generate_mnist.py noniid - dir # for practical noniid and unbalanced scenario
    ```

The output of `generate_mnist.py iid - -`
```
Original number of samples of each label: [6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958]

Client 0     Size of data: 1064  Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
Client 0     Samples of labels:  [(0, 101), (1, 128), (2, 136), (3, 123), (4, 79), (5, 85), (6, 107), (7, 127), (8, 74), (9, 104)]
--------------------------------------------------
Client 1     Size of data: 1023  Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
Client 1     Samples of labels:  [(0, 76), (1, 132), (2, 107), (3, 79), (4, 94), (5, 110), (6, 90), (7, 110), (8, 92), (9, 133)]
--------------------------------------------------
Client 2     Size of data: 923   Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
Client 2     Samples of labels:  [(0, 136), (1, 89), (2, 84), (3, 88), (4, 78), (5, 124), (6, 120), (7, 66), (8, 69), (9, 69)]
--------------------------------------------------
```
<details>
    <summary>Show more</summary>

    Client 3     Size of data: 906   Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
    Client 3     Samples of labels:  [(0, 73), (1, 151), (2, 94), (3, 73), (4, 83), (5, 67), (6, 133), (7, 92), (8, 69), (9, 71)]
    --------------------------------------------------
    Client 4     Size of data: 1045  Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
    Client 4     Samples of labels:  [(0, 69), (1, 71), (2, 100), (3, 130), (4, 90), (5, 120), (6, 116), (7, 142), (8, 106), (9, 101)]
    --------------------------------------------------
    Client 5     Size of data: 1026  Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
    Client 5     Samples of labels:  [(0, 128), (1, 90), (2, 71), (3, 135), (4, 71), (5, 88), (6, 91), (7, 139), (8, 116), (9, 97)]
    --------------------------------------------------
    Client 6     Size of data: 1033  Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
    Client 6     Samples of labels:  [(0, 80), (1, 89), (2, 109), (3, 117), (4, 117), (5, 80), (6, 107), (7, 122), (8, 121), (9, 91)]
    --------------------------------------------------
    Client 7     Size of data: 1043  Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
    Client 7     Samples of labels:  [(0, 65), (1, 86), (2, 132), (3, 133), (4, 111), (5, 110), (6, 65), (7, 106), (8, 120), (9, 115)]
    --------------------------------------------------
    Client 8     Size of data: 1019  Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
    Client 8     Samples of labels:  [(0, 135), (1, 73), (2, 121), (3, 100), (4, 124), (5, 118), (6, 90), (7, 90), (8, 74), (9, 94)]
    --------------------------------------------------
    Client 9     Size of data: 938   Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
    Client 9     Samples of labels:  [(0, 70), (1, 131), (2, 77), (3, 85), (4, 98), (5, 79), (6, 94), (7, 85), (8, 112), (9, 107)]
    --------------------------------------------------
    Client 10    Size of data: 964   Labels:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
    Client 10    Samples of labels:  [(0, 89), (1, 87), (2, 74), (3, 104), (4, 96), (5, 71), (6, 128), (7, 122), (8, 83), (9, 110)]

    Finish generating dataset.
</details>
<br/>

The output of `generate_mnist.py noniid - pat`
```
Original number of samples of each label: [6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958]

Client 0     Size of data: 799   Labels:  [0. 1.]
Client 0     Samples of labels:  [(0, 141), (1, 658)]
--------------------------------------------------
Client 1     Size of data: 687   Labels:  [0. 1.]
Client 1     Samples of labels:  [(0, 106), (1, 581)]
--------------------------------------------------
Client 2     Size of data: 4649  Labels:  [0. 1.]
Client 2     Samples of labels:  [(0, 3903), (1, 746)]
--------------------------------------------------
```
<details>
    <summary>Show more</summary>

    Client 3     Size of data: 853   Labels:  [0. 1.]
    Client 3     Samples of labels:  [(0, 213), (1, 640)]
    --------------------------------------------------
    Client 4     Size of data: 826   Labels:  [0. 1.]
    Client 4     Samples of labels:  [(0, 350), (1, 476)]
    --------------------------------------------------
    Client 5     Size of data: 1133  Labels:  [0. 1.]
    Client 5     Samples of labels:  [(0, 577), (1, 556)]
    --------------------------------------------------
    Client 6     Size of data: 752   Labels:  [0. 1.]
    Client 6     Samples of labels:  [(0, 459), (1, 293)]
    --------------------------------------------------
    Client 7     Size of data: 523   Labels:  [0. 1.]
    Client 7     Samples of labels:  [(0, 304), (1, 219)]
    --------------------------------------------------
    Client 8     Size of data: 362   Labels:  [0. 1.]
    Client 8     Samples of labels:  [(0, 198), (1, 164)]
    --------------------------------------------------
    Client 9     Size of data: 4196  Labels:  [0. 1.]
    Client 9     Samples of labels:  [(0, 652), (1, 3544)]
    --------------------------------------------------
    Client 10    Size of data: 542   Labels:  [2. 3.]
    Client 10    Samples of labels:  [(2, 456), (3, 86)]
    
    Finish generating dataset.
</details>
<br/>

The output of `generate_mnist.py noniid - dir` (`alpha = 0.1` for the Dirichlet distribution in `./dataset/utils/dataset_utils.py`)
```
Original number of samples of each label: [6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958]

Client 0         Size of data: 1059      Labels:  [1. 3. 4. 6. 8.]
Client 0         Samples of labels:  [(1, 71), (3, 98), (4, 228), (6, 577), (8, 85)]
--------------------------------------------------
Client 1         Size of data: 1138      Labels:  [2. 3. 4. 7. 8.]
Client 1         Samples of labels:  [(2, 198), (3, 138), (4, 201), (7, 515), (8, 86)]
--------------------------------------------------
Client 2         Size of data: 755       Labels:  [0. 1. 3. 7. 8.]
Client 2         Samples of labels:  [(0, 75), (1, 107), (3, 130), (7, 291), (8, 152)]
--------------------------------------------------
```
<details>
    <summary>Show more</summary>

    Client 3         Size of data: 875       Labels:  [1. 3. 5. 7.]
    Client 3         Samples of labels:  [(1, 254), (3, 74), (5, 160), (7, 387)]
    --------------------------------------------------
    Client 4         Size of data: 4228      Labels:  [0. 2. 4. 5. 7. 8.]
    Client 4         Samples of labels:  [(0, 77), (2, 276), (4, 173), (5, 483), (7, 3087), (8, 132)]
    --------------------------------------------------
    Client 5         Size of data: 800       Labels:  [0. 1. 2. 3. 4. 8.]
    Client 5         Samples of labels:  [(0, 140), (1, 269), (2, 120), (3, 94), (4, 77), (8, 100)]
    --------------------------------------------------
    Client 6         Size of data: 3286      Labels:  [0. 1. 2. 3. 4. 8.]
    Client 6         Samples of labels:  [(0, 2434), (1, 213), (2, 281), (3, 132), (4, 117), (8, 109)]
    --------------------------------------------------
    Client 7         Size of data: 413       Labels:  [2. 3. 4. 8.]
    Client 7         Samples of labels:  [(2, 160), (3, 80), (4, 87), (8, 86)]
    --------------------------------------------------
    Client 8         Size of data: 641       Labels:  [1. 3. 7. 8.]
    Client 8         Samples of labels:  [(1, 129), (3, 127), (7, 238), (8, 147)]
    --------------------------------------------------
    Client 9         Size of data: 3359      Labels:  [0. 2. 3. 6. 8.]
    Client 9         Samples of labels:  [(0, 132), (2, 263), (3, 69), (6, 2791), (8, 104)]
    --------------------------------------------------
    Client 10        Size of data: 461       Labels:  [0. 3. 4. 8.]
    Client 10        Samples of labels:  [(0, 171), (3, 96), (4, 103), (8, 91)]
  
    
    Finish generating dataset.
</details>

## Models
- for MNIST and Fashion-MNIST

    1. Mclr_Logistic(1\*28\*28)
    2. LeNet()
    3. DNN(1\*28\*28, 100) # non-convex

- for Cifar10, Cifar100 and Tiny-ImageNet

    1. Mclr_Logistic(3\*32\*32)
    2. FedAvgCNN()
    3. DNN(3\*32\*32, 100) # non-convex
    4. ResNet18, AlexNet, MobileNet, GoogleNet, etc.

- for AG_News and Sogou_News

    1. LSTM()
    2. fastText() in [Bag of Tricks for Efficient Text Classification](https://aclanthology.org/E17-2068/) 
    3. TextCNN() in [Convolutional Neural Networks for Sentence Classification](https://aclanthology.org/D14-1181/)
    4. TransformerModel() in [Attention is all you need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)

- for AmazonReview

    1. AmazonMLP() in [Curriculum manager for source selection in multi-source domain adaptation](https://link.springer.com/chapter/10.1007/978-3-030-58568-6_36)

- for Omniglot

    1. FedAvgCNN()

- for HAR and PAMAP

    1. HARCNN() in [Convolutional neural networks for human activity recognition using mobile sensors](https://eudl.eu/pdf/10.4108/icst.mobicase.2014.257786)

## Environments
Install [CUDA](https://developer.nvidia.com/cuda-11-6-0-download-archive) first. 

With the installed [conda](https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh), we can run this platform in a conda virtual environment called *fl_torch*. 

```bash
conda env create -f env_cuda_latest.yaml # You may need to downgrade the torch using pip to match CUDA version
```

## How to start simulating (examples for FedAvg)

![](./structure.png)

- Build dataset: [Datasets](#Datasets-and-Separation-(updating))

- Train and evaluate the model:
    ```bash
    cd ./system
    python main.py -data mnist -m cnn -algo FedAvg -gr 2500 -did 0 -go cnn # for FedAvg and MNIST
    ```
    Or you can uncomment the lines you need in `./system/examples.sh` and run:
    ```bash
    cd ./system
    sh examples.sh
    ```

**Note**: The hyper-parameters have not been tuned for the algorithms. The values in `./system/examples.sh` are just examples. You need to tune the hyper-parameters by yourself. 

## Practical scenario
If you need to simulate FL in a practical scenario, which includes **client dropout**, **slow trainers**, **slow senders**, and **network TTL**, you can set the following parameters to realize it.

- `-cdr`: The dropout rate for total clients. The selected clients will randomly drop at each training round.
- `-tsr` and `-ssr`: The rates for slow trainers and slow senders among all clients. Once a client was selected as "slow trainers", for example, it will always train slower than the original one. So does "slow senders". 
- `-tth`: The threshold for network TTL (ms). 

## Easy to extend
It is easy to add new datasets or FL algorithms to this platform. 

- To add a **new dataset** into this platform, all you need to do is writing the download code and using the utils the same as `./dataset/generate_mnist.py` (you can also consider it as the template). 

- To add a **new algorithm**, you can utilize the class **Server** and class **Client**, which are wrote in `./system/flcore/servers/serverbase.py` and `./system/flcore/clients/clientbase.py`, respectively. 

- To add a **new model**, just add it into `./system/flcore/trainmodel/models.py`.

- If you have a **new optimizer** while training, please add it into `./system/flcore/optimizers/fedoptimizer.py`

- This platform is also convenient for users to bulid a new platform for specific applications, such as our [FL-IoT](https://github.com/TsingZ0/FL-IoT) and [HtFL](https://github.com/TsingZ0/HtFL). 


## Experimental results

If you are interested in **the experimental results(e.g., the accuracy) of above algorithms**, you can find some results in our accepted FL papers (i.e., [FedALA](https://github.com/TsingZ0/FedALA), [FedCP](https://github.com/TsingZ0/FedCP), and [GPFL](https://github.com/TsingZ0/GPFL)) listed as follows that also use this platform. 

```
@inproceedings{zhang2023fedala,
  title={Fedala: Adaptive local aggregation for personalized federated learning},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={11237--11244},
  year={2023}
}

@inproceedings{Zhang2023fedcp,
  author = {Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  title = {FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy},
  year = {2023},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}
}

@inproceedings{zhang2023gpfl,
  title={GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Cao, Jian and Guan, Haibing},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5041--5051},
  year={2023}
}
```
